---
title: "Regression for binary DVs"
author: "Mark Deming"
date: "3/04/2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)



```

<style>
h1 {
    font-size: 24px;
}
h2 {
    font-size: 18px;
}
h3 {
    font-size: 14px;
}
</style>


# Regression for binary DVs

Often, we wish to apply regression to a binary dependent variable--an outcome coded as 0 or 1. For example:

- Civil war onset
- Transition to democracy
- Individual turnout in an election

We have three common regression models for binary outcomes:

- Linear probability model
- Probit regression
- Logit regression

Here is an overview of the key advantages and disadvantage of each of these models:

```{r echo = FALSE}
# packages
library(knitr)
library(kableExtra)

# data frame
df <- data.frame(
  Model = c("LPM", "Probit", "Logit"),
  Advantages = c(
    "Easy to estimate and interpret; coefficients are changes in probability",
    "Proper probability model; flexible nonlinearity",
    "Proper probability model; odds ratio interpretation"
  ),
  Disadvantages = c(
    "Predicted probabilities can fall outside [0,1]; assumes constant effect of X",
    "Coefficients are changes in a latent z-score, not probabilities",
    "Log-odds can be less intuitive than probabilities"
  ),
  Interpretation = c(
    "Change in probability per 1-unit change in X",
    "Use marginal effects for interpretation",
    "Change in log-odds per 1-unit change in X"
  )
)

# send to kable()
kable(df, format = "markdown", escape = FALSE) %>%
  kable_styling(full_width = FALSE, position = "center", bootstrap_options = c("striped", "hover")) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, width = "8em") %>%
  column_spec(2:4, width = "15em") %>%
  row_spec(1:nrow(df), extra_css = "height: 2em;")
```

# Linear probability model (LPM)
LPMs are no different than OLS. We are simply applying OLS to a binary DV. This changes our interpretation of the regression coefficient:

"A one-unit increase in $X$ is associated with a coefficient-sized increase in the *probability* that $Y = 1$."


```{r echo = FALSE}
lpm_table <- data.frame(
  Category = c("Advantages", "Disadvantages", "Interpretation"),
  Details = c(
    "Easy to estimate and interpret; coefficients are changes in probability",
    "Predicted probabilities can fall outside [0,1]; assumes constant effect of X; errors are heteroskedastic",
    "A 1-unit increase in X changes the probability of Y = 1 by the coefficient value"
  )
)

kable(lpm_table, format = "markdown", col.names = c("Category", "LPM Summary"))
```

## When to use LPM
- Quick initial examination of the IV-DV relationship. 

- You mainly care about *approximate* marginal effects and your goal is description rather than precise modeling of the probability distribution.

## Example in R

The example below is from *Introduction to Econometrics in R* [(LINK)](https://www.econometrics-with-r.org/11-rwabdv.html).
```{r message = FALSE}
# packages
library(AER)
library(stargazer)
library(ggplot2)

# data
data(HMDA)
```

```{r}
# convert 'deny' to numeric
HMDA$deny <- as.numeric(HMDA$deny) - 1

# estimate a simple linear probabilty model
denymod1 <- lm(deny ~ pirat, data = HMDA)
summary(denymod1)

# get robust standard errors
coeftest(denymod1, vcov. = vcovHC, type = "HC1")
```

Note that above, we calculated robust standard errors. This is required for PLMs, since the error terms are *always* heteroskedastic.

Examining the regression output, we see that the coefficient for `pirat` is 0.604. We interpret this as: A one-unit increase in the price-to-income ratio is associated with a 6% increase in the probability of loan denial.

## Advantages & disadvantages
The main advantage of PLMs is their simplicity and easy interpretation. The main disadvantage is that they assume constant marginal effects for all values of $X$ (a straight line), which is unrealistic for many binary outcomes. 

# Probit regression
Probit and Logit (below) models are designed for binary dependent variables. They ensure that predicted probabilities fall between 0 and 1. Both models link predictors to the probability of $Y = 1$ using an S-shaped curve. Probit relies on the normal distribution. Logit uses the logistic distribution.

Probit and logit both use **maximum likelihood estimation (MLE)**. MLE is just a method for finding the best-fitting coefficients in a regression model when the dependent variable is binary (like voted = 1, didn’t vote = 0). The basic idea is simple: MLE chooses the coefficients that make the observed data most likely to occur under the model. In other words, it finds the values that make the model’s predictions match the actual data as closely as possible — making the data look least surprising given the model.

```{r echo = FALSE}
probit_table <- data.frame(
  Category = c("Advantages", "Disadvantages", "Interpretation"),
  Details = c(
    "Proper probability model; flexible nonlinearity",
    "Coefficients are changes in a latent z-score, not probabilities",
    "Requires calculation of marginal effects for interpretation"
  )
)

kable(probit_table, format = "markdown", col.names = c("Category", "Probit Summary"))
```


## When to use Probit
- You want a proper probability model, but interpretation of coefficients is not the primary concern.
- You think that the latent process driving the outcome is assumed to be normally distributed.

## Example in R
```{r}
# estimate the simple probit model
denyprobit <- glm(deny ~ pirat, 
                  family = binomial(link = "probit"), 
                  data = HMDA)

coeftest(denyprobit, vcov. = vcovHC, type = "HC1")
```

The coefficient above is not directly interpretable. To help readers interpret the coefficient, we usually calculate predicted changes in probability as $X$ increases/decreases:

```{r}
# 1. compute predictions for P/I ratio = 0.3, 0.4
predictions <- predict(denyprobit, 
                       newdata = data.frame("pirat" = c(0.3, 0.4)),
                       type = "response")

# Above, note how line 2 creates a new data frame 
# with "fake" values of X: i.e., What is the predicted

# 2. Compute difference in probabilities
diff(predictions)
```

We find that an increase in the payment-to-income ratio from 0.3 to 0.4 is predicted to increase the probability of denial by approximately 6.1%.

# Logit regression

```{r echo = FALSE}
logit_table <- data.frame(
  Category = c("Advantages", "Disadvantages", "Interpretation"),
  Details = c(
    "Proper probability model; odds ratio interpretation possible",
    "Log-odds can be less intuitive than probabilities",
    "A 1-unit increase in X changes the log-odds of Y = 1 by the coefficient value (can convert to odds ratio)"
  )
)

kable(logit_table, format = "markdown", col.names = c("Category", "Logit Summary"))
```

## When to use Logit
- When you want a proper probability model with interpretable (sort of) coefficients (log-odds or odds ratios).
- When you believe odds-based interpretation fits your field (e.g., public health).

## Example in R
```{r}
denylogit <- glm(deny ~ pirat, 
                 family = binomial(link = "logit"), 
                 data = HMDA)

coeftest(denylogit, vcov. = vcovHC, type = "HC1")
```

# Final thoughts
In sum, we like PLM because they are simple and easy to interpret. We don't like them because they assume a linear relationship between the IV and DV. 

By contrast, we like Probit and Logit because they model the IV-DV relationship as an s-shaped line. The idea is that the effect of the IV on the DV changes for different values of the IV. We don't like them because coefficients are not readily interpretable. We often calculate and discuss marginal effects to help readers better grasp the meaning of Probit and Logit regression output.

From *Introduction to Econometrics in R*:

The Probit model and the Logit model deliver only approximations to the unknown population regression function $E(Y|X$. It is not obvious how to decide which model to use in practice. The linear probability model has the clear drawback of not being able to capture the nonlinear nature of the population regression function and it may predict probabilities to lie outside the interval $[0, 1]$. Probit and Logit models are harder to interpret but capture the nonlinearities better than the linear approach: both models produce predictions of probabilities that lie inside the interval $[0, 1]$. Predictions of all three models are often close to each other. The book suggests to use the method that is easiest to use in the statistical software of choice. As we have seen, it is equally easy to estimate Probit and Logit model using R. We can therefore give no general recommendation which method to use.

# LPMs, Probit, and Logit in political science

## LPM
- Ansolabehere, S., & Konisky, D. M. (2006). The introduction of voter registration and its effect on turnout. Political Analysis, 14(1), 83-100.
- Ferree, K. E. (2006). Explaining South Africa's racial census. Journal of Politics, 68(4), 803-815.

## Probit
- Albertus, M. (2019). The fate of former authoritarian elites under democracy. Journal of Conflict Resolution, 63(3), 702-730.
- Albertus, M., & Menaldo, V. (2012). Coercive capacity and the prospects for democratization. Comparative Politics, 44(2), 151-169.
- Powell, G. B., & Whitten, G. D. (1993). A cross-national analysis of economic voting: Taking account of the political context. American Journal of Political Science, 37(2), 391-414.

## Logit
- Haggard, S., & Kaufman, R. R. (1997). The political economy of democratic transitions. Comparative Politics, 29(3), 263-283.
- Rosenstone, S. J., & Hansen, J. M. (1993). Mobilization, participation, and democracy in America. Macmillan.
- Trejo, G. (2009). Religious competition and ethnic mobilization in Latin America: Why the Catholic Church promotes indigenous rights in Mexico. American Political Science Review, 103(3), 323-342.

